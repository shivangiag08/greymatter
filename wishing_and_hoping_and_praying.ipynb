{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load('C:\\Shivangi\\college\\Sem 5\\Deep Learning\\DL project/train_features.npy')\n",
    "test = np.load('C:\\Shivangi\\college\\Sem 5\\Deep Learning\\DL project/test_features.npy')\n",
    "train10 = np.load('C:\\Shivangi\\college\\Sem 5\\Deep Learning\\DL project/train10_features.npy')\n",
    "test10 = np.load('C:\\Shivangi\\college\\Sem 5\\Deep Learning\\DL project/test10_features.npy')\n",
    "\n",
    "# #concatenate train and train10  \n",
    "train = np.concatenate((train, train10), axis=0)\n",
    "test = np.concatenate((test, test10), axis=0)\n",
    "\n",
    "# write to npy\n",
    "np.save('C:\\Shivangi\\college\\Sem 5\\Deep Learning\\DL project/train_features_all.npy', train)\n",
    "np.save('C:\\Shivangi\\college\\Sem 5\\Deep Learning\\DL project/test_features_all.npy', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1301, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:00<00:00, 51.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2732, 6)\n",
      "(6692, 6)\n",
      "(11303, 6)\n",
      "(12701, 6)\n",
      "(13953, 6)\n",
      "(18106, 6)\n",
      "(19735, 6)\n",
      "(21254, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 23.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63026, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "csv_data = pd.read_csv(r\"C:\\Shivangi\\college\\Sem 5\\Deep Learning\\DL project\\zips\\30_patients_zip1.csv\")\n",
    "print(csv_data.shape)\n",
    "for i in tqdm(range(2, 11)):\n",
    "    csv = pd.read_csv(f\"C:\\Shivangi\\college\\Sem 5\\Deep Learning\\DL project\\zips/30_patients_zip{i}.csv\")\n",
    "    csv_data = pd.concat([csv_data, csv], axis=0)\n",
    "    print(csv_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((63026, 6), (41732, 2048), (40, 2048), (62998, 2048), (10661, 2048))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data.shape, train10.shape, test10.shape, train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_patient_ids = [\n",
    "    \"098_S_4018\", \"098_S_4017\", \"116_S_1271\", \"031_S_0294\", \"031_S_4021\", \n",
    "    \"023_S_4020\", \"031_S_4024\", \"099_S_4022\", \"116_S_4010\", \"037_S_4028\",\n",
    "    \"024_S_4084\", \"067_S_4782\", \"011_S_4827\", \"014_S_2185\", \"014_S_4401\", \n",
    "    \"022_S_6069\", \"041_S_4060\", \"041_S_4138\", \"041_S_4143\", \"041_S_4874\",\n",
    "    \"011_S_0002\", \"011_S_0003\", \"011_S_0005\", \"011_S_0008\", \"022_S_0007\", \n",
    "    \"100_S_0015\", \"023_S_0030\", \"023_S_0031\", \"011_S_0016\", \"073_S_4393\",\n",
    "    '941_S_6499', '016_S_6931' ,'018_S_2155' ,'082_S_1119', '027_S_0835','116_S_1243'\n",
    "]\n",
    "\n",
    "train_patient_ids = selected_patient_ids[:30]  # First 30 for training\n",
    "test_patient_ids = selected_patient_ids[30:]   # Last 6 for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the csv data into train and test and save\n",
    "train_csv = csv_data[csv_data['patient_id'].isin(train_patient_ids)]\n",
    "test_csv = csv_data[csv_data['patient_id'].isin(test_patient_ids)]\n",
    "\n",
    "train_csv.to_csv('C:\\Shivangi\\college\\Sem 5\\Deep Learning\\DL project/zips/train_labels_all.csv', index=False)\n",
    "test_csv.to_csv('C:\\Shivangi\\college\\Sem 5\\Deep Learning\\DL project/zips/test_labels_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10661, 2048), (10661, 6))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape, test_csv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the features into train and test and save\n",
    "train_features = train[train_csv.index]\n",
    "test_csv = test_csv.reset_index(drop=True)\n",
    "test_features = test[test_csv.index]\n",
    "\n",
    "np.save('C:\\Shivangi\\college\\Sem 5\\Deep Learning\\DL project/train_features_all.npy', train_features)\n",
    "np.save('C:\\Shivangi\\college\\Sem 5\\Deep Learning\\DL project/test_features_all.npy', test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded features from train_features_all.npy, shape: (52365, 2048)\n",
      "Loaded features from test_features_all.npy, shape: (10661, 2048)\n",
      "Loaded CSV train_labels_all.csv, shape: (52365, 6)\n",
      "Loaded CSV test_labels_all.csv, shape: (10661, 6)\n",
      "Combined train and test data, shape: (63026, 8)\n",
      "Loaded CSV Cohort_4_MRI_Images_02Dec2024.csv, shape: (17283, 23)\n",
      "Merged image metadata, shape: (861198, 13)\n",
      "Loaded CSV ADAS_scores.csv, shape: (15480, 15)\n",
      "Loaded CSV MMSE_scores.csv, shape: (13800, 53)\n",
      "Loaded CSV APOE.csv, shape: (2758, 7)\n",
      "Loaded CSV DXSUM_04Dec2024.csv, shape: (14333, 41)\n",
      "Merged ADAS, MMSE, Biomarker, Diagnosis, shape: (861198, 18)\n",
      "Train merged saved to C:\\Shivangi\\college\\Sem 5\\Deep Learning\\DL project\\zips\\train_merged.csv, shape: (783419, 18)\n",
      "Test merged saved to C:\\Shivangi\\college\\Sem 5\\Deep Learning\\DL project\\zips\\test_merged.csv, shape: (77779, 18)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# ############################### Configuration ###################################\n",
    "'''\n",
    "Update paths according to your directory structure.\n",
    "'''\n",
    "\n",
    "TRAIN_FEATURES_PATH = r\"C:\\Shivangi\\college\\Sem 5\\Deep Learning\\DL project\\train_features_all.npy\"\n",
    "TEST_FEATURES_PATH = r\"C:\\Shivangi\\college\\Sem 5\\Deep Learning\\DL project\\test_features_all.npy\"\n",
    "\n",
    "TRAIN_CSV_PATH = r\"C:\\Shivangi\\college\\Sem 5\\Deep Learning\\DL project\\zips\\train_labels_all.csv\"\n",
    "TEST_CSV_PATH = r\"C:\\Shivangi\\college\\Sem 5\\Deep Learning\\DL project\\zips\\test_labels_all.csv\"\n",
    "\n",
    "ADAS_CSV_PATH = r\"C:\\Shivangi\\college\\Sem 5\\Deep Learning\\DL project\\greymatter\\text_data\\ADAS_scores.csv\"\n",
    "MMSE_CSV_PATH = r\"C:\\Shivangi\\college\\Sem 5\\Deep Learning\\DL project\\greymatter\\text_data\\MMSE_scores.csv\"\n",
    "BIOMARKER_CSV_PATH = r\"C:\\Shivangi\\college\\Sem 5\\Deep Learning\\DL project\\greymatter\\text_data\\APOE.csv\"\n",
    "\n",
    "IMAGE_METADATA_CSV_PATH = r\"F:\\DL_DATASET\\Cohort_4_MRI_Images_02Dec2024.csv\"\n",
    "DIAGNOSIS_CSV_PATH = r\"F:\\DL_DATASET\\DXSUM_04Dec2024.csv\"\n",
    "\n",
    "TRAIN_OUTPUT_PATH = r\"C:\\Shivangi\\college\\Sem 5\\Deep Learning\\DL project\\zips\\train_merged.csv\"\n",
    "TEST_OUTPUT_PATH = r\"C:\\Shivangi\\college\\Sem 5\\Deep Learning\\DL project\\zips\\test_merged.csv\"\n",
    "\n",
    "selected_patient_ids = [\n",
    "    \"098_S_4018\", \"098_S_4017\", \"116_S_1271\", \"031_S_0294\", \"031_S_4021\", \n",
    "    \"023_S_4020\", \"031_S_4024\", \"099_S_4022\", \"116_S_4010\", \"037_S_4028\",\n",
    "    \"024_S_4084\", \"067_S_4782\", \"011_S_4827\", \"014_S_2185\", \"014_S_4401\", \n",
    "    \"022_S_6069\", \"041_S_4060\", \"041_S_4138\", \"041_S_4143\", \"041_S_4874\",\n",
    "    \"011_S_0002\", \"011_S_0003\", \"011_S_0005\", \"011_S_0008\", \"022_S_0007\", \n",
    "    \"100_S_0015\", \"023_S_0030\", \"023_S_0031\", \"011_S_0016\", \"073_S_4393\",\n",
    "    \"941_S_6499\", \"016_S_6931\", \"018_S_2155\", \"082_S_1119\", \"027_S_0835\", \"116_S_1243\"\n",
    "]\n",
    "\n",
    "train_patient_ids = selected_patient_ids[:30]\n",
    "test_patient_ids = selected_patient_ids[30:]\n",
    "\n",
    "# ############################### Functions ########################################\n",
    "\n",
    "def load_features(features_path):\n",
    "    if os.path.exists(features_path):\n",
    "        features = np.load(features_path)\n",
    "        print(f\"Loaded features from {os.path.basename(features_path)}, shape: {features.shape}\")\n",
    "        return features\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Features file not found: {features_path}\")\n",
    "\n",
    "def load_csv(csv_path):\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Loaded CSV {os.path.basename(csv_path)}, shape: {df.shape}\")\n",
    "        return df\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n",
    "\n",
    "def main():\n",
    "    # Load features (just to ensure matching rows, not to merge them)\n",
    "    train_features = load_features(TRAIN_FEATURES_PATH)\n",
    "    test_features = load_features(TEST_FEATURES_PATH)\n",
    "    \n",
    "    # Load CSV files\n",
    "    train_csv = load_csv(TRAIN_CSV_PATH)\n",
    "    test_csv = load_csv(TEST_CSV_PATH)\n",
    "    \n",
    "    # Check feature and CSV row counts\n",
    "    if train_features.shape[0] != len(train_csv):\n",
    "        raise ValueError(f\"Train features ({train_features.shape[0]}) != train CSV rows ({len(train_csv)})\")\n",
    "    if test_features.shape[0] != len(test_csv):\n",
    "        raise ValueError(f\"Test features ({test_features.shape[0]}) != test CSV rows ({len(test_csv)})\")\n",
    "\n",
    "    # Add row_id for linking to features later\n",
    "    train_csv['row_id'] = np.arange(len(train_csv))\n",
    "    test_csv['row_id'] = np.arange(len(test_csv))\n",
    "\n",
    "    # Add a 'set' column\n",
    "    train_csv['set'] = 'train'\n",
    "    test_csv['set'] = 'test'\n",
    "\n",
    "    combined_df = pd.concat([train_csv, test_csv], axis=0).reset_index(drop=True)\n",
    "    print(f\"Combined train and test data, shape: {combined_df.shape}\")\n",
    "\n",
    "    # Rename columns for consistent keys: patient_id -> PTID, datetime -> VISDATE if they exist\n",
    "    if 'patient_id' in combined_df.columns:\n",
    "        combined_df.rename(columns={'patient_id': 'PTID'}, inplace=True)\n",
    "    if 'datetime' in combined_df.columns:\n",
    "        combined_df.rename(columns={'datetime': 'VISDATE'}, inplace=True)\n",
    "\n",
    "    # Convert VISDATE to datetime and then to just date\n",
    "    if 'VISDATE' in combined_df.columns:\n",
    "        combined_df['VISDATE'] = pd.to_datetime(combined_df['VISDATE']).dt.date\n",
    "\n",
    "    # Load image metadata\n",
    "    image_metadata = load_csv(IMAGE_METADATA_CSV_PATH)\n",
    "    image_metadata = image_metadata[['image_id', 'subject_id', 'mri_visit', 'mri_date', 'mri_type', 'mri_weighting', 'mri_acq_plane']]\n",
    "    image_metadata['mri_date'] = pd.to_datetime(image_metadata['mri_date']).dt.date\n",
    "    image_metadata.rename(columns={'subject_id':'PTID', 'mri_date':'VISDATE'}, inplace=True)\n",
    "\n",
    "    # Merge image metadata\n",
    "    merged_df = pd.merge(combined_df, image_metadata, on=['PTID','VISDATE'], how='left')\n",
    "    print(f\"Merged image metadata, shape: {merged_df.shape}\")\n",
    "\n",
    "    # Load ADAS, MMSE, Biomarker, Diagnosis\n",
    "    adas_df = load_csv(ADAS_CSV_PATH)[['PTID', 'VISDATE', 'TOTAL13']]\n",
    "    mmse_df = load_csv(MMSE_CSV_PATH)[['PTID', 'VISDATE', 'MMSCORE']]\n",
    "    biomarker_df = load_csv(BIOMARKER_CSV_PATH)[['PTID', 'APTESTDT', 'GENOTYPE', 'APUSABLE']]\n",
    "    diagnosis_df = load_csv(DIAGNOSIS_CSV_PATH)[['PTID', 'EXAMDATE', 'DIAGNOSIS']]\n",
    "\n",
    "    # Process dates for merging\n",
    "    adas_df['VISDATE'] = pd.to_datetime(adas_df['VISDATE']).dt.date\n",
    "    mmse_df['VISDATE'] = pd.to_datetime(mmse_df['VISDATE']).dt.date\n",
    "    biomarker_df['APTESTDT'] = pd.to_datetime(biomarker_df['APTESTDT']).dt.date\n",
    "    biomarker_df.rename(columns={'APTESTDT':'VISDATE'}, inplace=True)\n",
    "    diagnosis_df['EXAMDATE'] = pd.to_datetime(diagnosis_df['EXAMDATE']).dt.date\n",
    "    diagnosis_df.rename(columns={'EXAMDATE':'VISDATE'}, inplace=True)\n",
    "\n",
    "    # Merge ADAS\n",
    "    merged_df = pd.merge(merged_df, adas_df, on=['PTID','VISDATE'], how='left')\n",
    "    # Merge MMSE\n",
    "    merged_df = pd.merge(merged_df, mmse_df, on=['PTID','VISDATE'], how='left')\n",
    "    # Merge Biomarker\n",
    "    merged_df = pd.merge(merged_df, biomarker_df, on=['PTID','VISDATE'], how='left')\n",
    "    # Merge Diagnosis\n",
    "    merged_df = pd.merge(merged_df, diagnosis_df, on=['PTID','VISDATE'], how='left')\n",
    "    print(f\"Merged ADAS, MMSE, Biomarker, Diagnosis, shape: {merged_df.shape}\")\n",
    "\n",
    "    # At this point, merged_df contains row_id, set, PTID, VISDATE, and all metadata.\n",
    "    # No features added from npy (just row_id).\n",
    "\n",
    "    # Separate back into train and test\n",
    "    train_merged = merged_df[merged_df['set'] == 'train'].reset_index(drop=True)\n",
    "    test_merged = merged_df[merged_df['set'] == 'test'].reset_index(drop=True)\n",
    "\n",
    "    # Optionally handle missing values here if needed\n",
    "    # Or just save directly\n",
    "    # For large scale: no memory error since no large arrays\n",
    "\n",
    "    # Save final train and test CSV files without large npy features\n",
    "    train_merged.to_csv(TRAIN_OUTPUT_PATH, index=False)\n",
    "    test_merged.to_csv(TEST_OUTPUT_PATH, index=False)\n",
    "    print(f\"Train merged saved to {TRAIN_OUTPUT_PATH}, shape: {train_merged.shape}\")\n",
    "    print(f\"Test merged saved to {TEST_OUTPUT_PATH}, shape: {test_merged.shape}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
