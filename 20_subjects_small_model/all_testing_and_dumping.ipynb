{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved\n",
      "  patient_id scan_type            datetime  \\\n",
      "0      003_S      6644 2018-12-06 18:58:32   \n",
      "1      005_S      4168 2011-08-17 16:28:48   \n",
      "2      005_S      4168 2011-08-17 16:28:48   \n",
      "3      005_S      4168 2011-08-17 16:28:48   \n",
      "4      005_S      4168 2012-03-28 12:58:15   \n",
      "\n",
      "                                            filename  \\\n",
      "0  ADNI_003_S_6644_MR_3_Plane_Localizer__raw_2018...   \n",
      "1  ADNI_005_S_4168_MR_3_Plane_Localizer__br_raw_2...   \n",
      "2  ADNI_005_S_4168_MR_3_Plane_Localizer__br_raw_2...   \n",
      "3  ADNI_005_S_4168_MR_3_Plane_Localizer__br_raw_2...   \n",
      "4  ADNI_005_S_4168_MR_3_Plane_Localizer__br_raw_2...   \n",
      "\n",
      "                                           file_path  \n",
      "0  F:\\DL_DATASET\\test_folder\\ADNI\\003_S_6644\\3_Pl...  \n",
      "1  F:\\DL_DATASET\\test_folder\\ADNI\\005_S_4168\\3_Pl...  \n",
      "2  F:\\DL_DATASET\\test_folder\\ADNI\\005_S_4168\\3_Pl...  \n",
      "3  F:\\DL_DATASET\\test_folder\\ADNI\\005_S_4168\\3_Pl...  \n",
      "4  F:\\DL_DATASET\\test_folder\\ADNI\\005_S_4168\\3_Pl...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def parse_dicom_filename(file_name):\n",
    "    # Regex to extract datetime in format YYYYMMDDHHMMSS\n",
    "    datetime_pattern = r\"(\\d{8})(\\d{6})\"  # Matches YYYYMMDDHHMMSS\n",
    "\n",
    "    # Split the filename by underscores ('_')\n",
    "    parts = file_name.split('_')\n",
    "\n",
    "    # Extract patient ID\n",
    "    patient_id = parts[1] + \"_\" + parts[2]  # e.g., 003_S_6644\n",
    "\n",
    "    # Extract scan type\n",
    "    scan_type = parts[3] if len(parts) >= 4 else None\n",
    "\n",
    "    # Use regex to find the datetime in the filename\n",
    "    match = re.search(datetime_pattern, file_name)\n",
    "    if match:\n",
    "        date_part = match.group(1)  # YYYYMMDD\n",
    "        time_part = match.group(2)  # HHMMSS\n",
    "        datetime_str = f\"{date_part[:4]}-{date_part[4:6]}-{date_part[6:8]} {time_part[:2]}:{time_part[2:4]}:{time_part[4:6]}\"\n",
    "        try:\n",
    "            datetime_obj = datetime.strptime(datetime_str, '%Y-%m-%d %H:%M:%S')\n",
    "        except ValueError:\n",
    "            datetime_obj = None\n",
    "    else:\n",
    "        datetime_obj = None\n",
    "\n",
    "    return {\n",
    "        'patient_id': patient_id,\n",
    "        'scan_type': scan_type,\n",
    "        'datetime': datetime_obj,\n",
    "        'filename': file_name\n",
    "    }\n",
    "\n",
    "def process_folders(base_dir):\n",
    "    # List to store parsed data\n",
    "    all_data = []\n",
    "\n",
    "    # Traverse directories with os.walk()\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        if 'I' in os.path.basename(root):  # Only look at folders containing DICOM files\n",
    "            # print(f'    Processing directory: {root}')\n",
    "            \n",
    "            # Loop through files and try parsing the DICOM filenames\n",
    "            for file_name in files:\n",
    "                if file_name.endswith('.dcm'):\n",
    "                    file_path = os.path.join(root, file_name)\n",
    "                    \n",
    "                    # Parse the filename and append to the data list\n",
    "                    parsed_data = parse_dicom_filename(file_name)\n",
    "                    if parsed_data:\n",
    "                        parsed_data['file_path'] = file_path  # Add full file path to the data\n",
    "                        all_data.append(parsed_data)\n",
    "\n",
    "    # Convert the data into a DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "\n",
    "    # Save the DataFrame to a CSV file for further analysis\n",
    "    df.to_csv(r\"C:\\Shivangi\\college\\Sem 5\\Deep Learning\\DL project\\test_metadata.csv\", index=False)\n",
    "    print(\"Metadata saved\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Set the base directory\n",
    "base_dir = r\"F:\\DL_DATASET\\test_folder\\ADNI\"  # Adjust this to your actual directory\n",
    "\n",
    "# Process the directories and get the DataFrame\n",
    "df = process_folders(base_dir)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to merged_mri_metadata.csv\n",
      "                                          dicom_file  patient_id    mri_date  \\\n",
      "0  ADNI_003_S_6644_MR_3_Plane_Localizer__raw_2018...  003_S_6644  2018-12-04   \n",
      "1  ADNI_005_S_4168_MR_3_Plane_Localizer__br_raw_2...  005_S_4168  2011-08-17   \n",
      "2  ADNI_005_S_4168_MR_3_Plane_Localizer__br_raw_2...  005_S_4168  2011-08-17   \n",
      "3  ADNI_005_S_4168_MR_3_Plane_Localizer__br_raw_2...  005_S_4168  2011-08-17   \n",
      "4  ADNI_005_S_4168_MR_3_Plane_Localizer__br_raw_2...  005_S_4168  2012-03-28   \n",
      "\n",
      "  mri_acq_plane    mri_description mri_type mri_sequence  mri_field_str  \n",
      "0      SAGITTAL  3 Plane Localizer       2D           GR            3.0  \n",
      "1      SAGITTAL  3 Plane Localizer       2D           GR            3.0  \n",
      "2      SAGITTAL  3 Plane Localizer       2D           GR            3.0  \n",
      "3      SAGITTAL  3 Plane Localizer       2D           GR            3.0  \n",
      "4       CORONAL  3 Plane Localizer       2D           GR            3.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load metadata CSV into pandas dataframe (the provided dataset)\n",
    "metadata_df = pd.read_csv(\"F:/DL_DATASET/Cohort_4_MRI_Images_02Dec2024.csv\")\n",
    "\n",
    "# Load the dicom_metadata CSV (already extracted from filenames)\n",
    "dicom_metadata_path = r\"C:\\Shivangi\\college\\Sem 5\\Deep Learning\\DL project\\test_metadata.csv\"\n",
    "dicom_metadata_df = pd.read_csv(dicom_metadata_path)\n",
    "\n",
    "# List to hold the matched results\n",
    "matches = []\n",
    "\n",
    "# Iterate over all DICOM files in the dicom_metadata.csv\n",
    "for index, dicom_row in dicom_metadata_df.iterrows():\n",
    "    # Extract image_id from the dicom file\n",
    "    dicom_file = dicom_row['filename']\n",
    "    image_id = dicom_file.split(\"I\")[-1].split(\".\")[0]\n",
    "    \n",
    "    # Match the image_id with metadata\n",
    "    metadata_row = metadata_df[metadata_df['image_id'] == int(image_id)]\n",
    "    \n",
    "    if not metadata_row.empty:\n",
    "        # Extract relevant details\n",
    "        patient_id = metadata_row['subject_id'].values[0]\n",
    "        mri_date = metadata_row['mri_date'].values[0]\n",
    "        mri_acq_plane = metadata_row['mri_acq_plane'].values[0]\n",
    "        mri_description = metadata_row['mri_description'].values[0]\n",
    "        mri_type = metadata_row['mri_type'].values[0]\n",
    "        mri_sequence = metadata_row['mri_sequence'].values[0]\n",
    "        mri_field_str = metadata_row['mri_field_str'].values[0]\n",
    "        \n",
    "        # Add matched data to the list\n",
    "        matches.append({\n",
    "            'dicom_file': dicom_file,\n",
    "            'patient_id': patient_id,\n",
    "            'mri_date': mri_date,\n",
    "            'mri_acq_plane': mri_acq_plane,\n",
    "            'mri_description': mri_description,\n",
    "            'mri_type': mri_type,\n",
    "            'mri_sequence': mri_sequence,\n",
    "            'mri_field_str': mri_field_str\n",
    "        })\n",
    "    else:\n",
    "        print(f\"No metadata found for {dicom_file}\")\n",
    "\n",
    "# Convert matches to a DataFrame and save to CSV\n",
    "matched_df = pd.DataFrame(matches)\n",
    "\n",
    "# Specify the path for the merged file\n",
    "merged_metadata_path = \"merged_mri_metadata.csv\"\n",
    "\n",
    "# Save the matched data\n",
    "matched_df.to_csv(merged_metadata_path, index=False)\n",
    "\n",
    "# Show a preview of the merged data\n",
    "print(f\"Merged data saved to {merged_metadata_path}\")\n",
    "print(matched_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DICOM files: 100%|██████████| 2503/2503 [00:48<00:00, 51.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Images saved as 'processed_images_zip2.npy'.\n"
     ]
    }
   ],
   "source": [
    "import pydicom\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Load the merged metadata file\n",
    "# Use a raw string to handle backslashes in the file path\n",
    "metadata_df = pd.read_csv(r'C:\\Shivangi\\college\\Sem 5\\Deep Learning\\DL project\\test_metadata.csv')\n",
    "\n",
    "def preprocess_single_frame(img, target_size):\n",
    "    # Resize the image by padding if necessary to maintain the aspect ratio\n",
    "    h, w = img.shape\n",
    "    scale = min(target_size[0] / h, target_size[1] / w)\n",
    "    new_w = int(w * scale)\n",
    "    new_h = int(h * scale)\n",
    "    \n",
    "    # Resize the image to the new dimensions\n",
    "    resized_img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    # Add padding to make it the target size\n",
    "    top = (target_size[0] - new_h) // 2\n",
    "    bottom = target_size[0] - new_h - top\n",
    "    left = (target_size[1] - new_w) // 2\n",
    "    right = target_size[1] - new_w - left\n",
    "    \n",
    "    padded_img = cv2.copyMakeBorder(resized_img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=0)\n",
    "    \n",
    "    return padded_img\n",
    "\n",
    "# Function to load DICOM images and preprocess them\n",
    "def load_and_preprocess_dicom(file_path, target_size=(256, 256)):\n",
    "    try:\n",
    "        # Read DICOM file\n",
    "        dicom_data = pydicom.dcmread(file_path)\n",
    "        \n",
    "        # Convert DICOM pixel data to numpy array\n",
    "        img = dicom_data.pixel_array\n",
    "        \n",
    "        # Handle multi-frame DICOMs\n",
    "        if img.ndim == 3:\n",
    "            # Assuming the first dimension is frames\n",
    "            frames = img.shape[0]\n",
    "            processed_frames = []\n",
    "            for i in range(frames):\n",
    "                single_frame = img[i]\n",
    "                # If the frame has multiple channels, convert to grayscale\n",
    "                if single_frame.ndim > 2:\n",
    "                    single_frame = np.mean(single_frame, axis=-1).astype(single_frame.dtype)\n",
    "                processed_frame = preprocess_single_frame(single_frame, target_size)\n",
    "                processed_frames.append(processed_frame)\n",
    "            return processed_frames  # Return list of processed frames\n",
    "        elif img.ndim == 2:\n",
    "            # Single-frame DICOM\n",
    "            processed_img = preprocess_single_frame(img, target_size)\n",
    "            return [processed_img]  # Return as a list for consistency\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported image dimensions: {img.shape}\")\n",
    "    except Exception as e:\n",
    "        # Raise the exception to be caught in the main loop\n",
    "        raise RuntimeError(f\"Failed to process {file_path}: {e}\")\n",
    "\n",
    "# Prepare the data\n",
    "processed_images = []\n",
    "failed_files = []\n",
    "\n",
    "# Loop through the metadata and process each file\n",
    "for idx, row in tqdm(metadata_df.iterrows(), total=metadata_df.shape[0], desc=\"Processing DICOM files\"):\n",
    "    dicom_file = row['file_path']\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(dicom_file):\n",
    "        print(f\"File does not exist: {dicom_file}\")\n",
    "        failed_files.append((dicom_file, \"File not found\"))\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        processed_imgs = load_and_preprocess_dicom(dicom_file)\n",
    "        processed_images.extend(processed_imgs)  # Add all frames to the list\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        failed_files.append((dicom_file, str(e)))\n",
    "\n",
    "# Convert list of images into numpy array\n",
    "if processed_images:\n",
    "    processed_images_array = np.array(processed_images)\n",
    "    \n",
    "    # Save the preprocessed images as .npy file\n",
    "    np.save(\"processed_images_zip2.npy\", processed_images_array)\n",
    "    print(\"Preprocessing complete. Images saved as 'processed_images_zip2.npy'.\")\n",
    "else:\n",
    "    print(\"No images were processed successfully.\")\n",
    "\n",
    "# Optionally, save the list of failed files for review\n",
    "if failed_files:\n",
    "    failed_df = pd.DataFrame(failed_files, columns=['file_path', 'error'])\n",
    "    failed_df.to_csv(\"D:/DL_DATASET/failed_files.csv\", index=False)\n",
    "    print(f\"{len(failed_files)} files failed to process. Details saved in 'failed_files.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Select 10 random indices from the processed images to visualize\n",
    "indices = np.random.choice(len(processed_images_array), 5, replace=False)\n",
    "\n",
    "# Plot the original vs processed images\n",
    "fig, axes = plt.subplots(5, 2, figsize=(10, 30))\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    # Load original DICOM image\n",
    "    dicom_file = metadata_df.iloc[idx]['file_path']\n",
    "    dicom_data = pydicom.dcmread(dicom_file)\n",
    "    original_img = dicom_data.pixel_array\n",
    "    \n",
    "    # Get the processed image\n",
    "    processed_img = processed_images_array[idx]\n",
    "    \n",
    "    # Plot original image\n",
    "    axes[i, 0].imshow(original_img, cmap='gray')\n",
    "    axes[i, 0].set_title(f\"Original Image {idx+1}\")\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Plot processed image\n",
    "    axes[i, 1].imshow(processed_img, cmap='gray')\n",
    "    axes[i, 1].set_title(f\"Processed Image {idx+1}\")\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load .npy file\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "processed_images_array = np.load(r\"C:\\Shivangi\\college\\Sem 5\\Deep Learning\\DL project\\train_features.npy\")\n",
    "csv_data = pd.read_csv(r\"C:\\Shivangi\\college\\Sem 5\\Deep Learning\\DL project\\zips\\30_patients_zip7.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_patient_ids = [\n",
    "    \"098_S_4018\", \"098_S_4017\", \"116_S_1271\", \"031_S_0294\", \"031_S_4021\", \n",
    "    \"023_S_4020\", \"031_S_4024\", \"099_S_4022\", \"116_S_4010\", \"037_S_4028\",\n",
    "    \"024_S_4084\", \"067_S_4782\", \"011_S_4827\", \"014_S_2185\", \"014_S_4401\", \n",
    "    \"022_S_6069\", \"041_S_4060\", \"041_S_4138\", \"041_S_4143\", \"041_S_4874\",\n",
    "    \"011_S_0002\", \"011_S_0003\", \"011_S_0005\", \"011_S_0008\", \"022_S_0007\", \n",
    "    \"100_S_0015\", \"023_S_0030\", \"023_S_0031\", \"011_S_0016\", \"073_S_4393\",\n",
    "    '941_S_6499', '016_S_6931' ,'018_S_2155' ,'082_S_1119', '027_S_0835','116_S_1243'\n",
    "]\n",
    "\n",
    "train_patient_ids = selected_patient_ids[:30]  # First 30 for training\n",
    "test_patient_ids = selected_patient_ids[30:]   # Last 6 for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['022_S_6069' '941_S_6499' '041_S_4874' '018_S_2155' '014_S_4401'\n",
      " '037_S_4028' '073_S_4393']\n"
     ]
    }
   ],
   "source": [
    "# tell me which subjects are present in the csv file\n",
    "print(csv_data['patient_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap between train_patient_ids and csv_data: {'022_S_6069', '041_S_4874', '014_S_4401', '037_S_4028', '073_S_4393'}\n",
      "Overlap between test_patient_ids and csv_data: {'018_S_2155', '941_S_6499'}\n"
     ]
    }
   ],
   "source": [
    "# count overlap between train_patient_ids and csv_data\n",
    "overlap = set(train_patient_ids).intersection(csv_data['patient_id'].unique())\n",
    "print(f\"Overlap between train_patient_ids and csv_data: {overlap}\")\n",
    "\n",
    "# count overlap between test_patient_ids and csv_data\n",
    "overlap = set(test_patient_ids).intersection(csv_data['patient_id'].unique())\n",
    "print(f\"Overlap between test_patient_ids and csv_data: {overlap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of csv data: (4153, 6)\n",
      "Size of npy data: (21266, 2048)\n"
     ]
    }
   ],
   "source": [
    "# count size of csv and npy files\n",
    "print(f\"Size of csv data: {csv_data.shape}\")\n",
    "print(f\"Size of npy data: {processed_images_array.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
